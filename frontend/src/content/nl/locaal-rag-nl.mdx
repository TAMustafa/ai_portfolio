<ProjectLayout
  title="Een veilig en lokaal LLM / RAG systeem"
  subtitle="A concise subtitle that expands on the value delivered"
  tags={["ChromaDB", "RAG", "Embeddings", "Langchain"]}
  >

In de huidige datagedreven wereld staan ​​organisaties voor twee hardnekkige uitdagingen:
het beheren van enorme hoeveelheden verspreide informatie en
het beschermen van gevoelige gegevens tegen het verlaten van beveiligde omgevingen.

Traditionele zoekmethoden schieten hier meestal tekort: ze retourneren ruwe documenten zonder context en lopen het risico vertrouwelijke gegevens bloot te leggen wanneer externe cloudservices betrokken zijn.

Dit project pakt deze problemen aan door lokale Large Language Models (LLM's) te combineren met Retrieval-Augmented Generation (RAG) – waardoor een systeem ontstaat dat zowel privacy-eerlijk als zeer nauwkeurig is.

## De oplossing: Lokale LLM's + RAG
Dit project laat zien hoe je een volledig lokale RAG-pipeline bouwt die ervoor zorgt dat alle gegevensverwerking intern plaatsvindt.

<Callout type="success" title="Data Security:">
Door een LLM lokaal uit te voeren met tools zoals **Ollama**, blijven gevoelige 
gegevens binnen de infrastructuur van de organisatie – zonder externe blootstelling. 
</Callout>

**Contextbewuste antwoorden:** De **RAG-pipeline** haalt relevante informatie op uit een lokale vectordatabase (ChromaDB) en voert deze in de LLM in, wat zorgt voor nauwkeurige, gecontextualiseerde antwoorden met duidelijke bronverwijzingen.

### Gebruikte technologie
Dit project bood een praktische ervaring met verschillende belangrijke technologieën:

- **<a href="https://fastapi.tiangolo.com/" target="_blank">FastAPI</a>:** Voor de backend-pipeline en het bedienen van de RAG-workflow.
- **<a href="https://langchain.com/" target="_blank">Langchain</a>:** Om het splitsen, insluiten en ophalen van documenten te orkestreren.
- **<a href="https://chromadb.io/" target="_blank">ChromaDB</a>:** Vectorstore voor snel en efficiënt zoeken naar overeenkomsten.
- **<a href="https://ollama.com/" target="_blank">Ollama</a>:** Werkt met lokale modellen zoals Mistral of Llama 2.
- **Embeddings:** nomic-embed-text (lokaal) of OpenAI's text-embedding-3-large.
- **Frontend:** React + TypeScript voor een overzichtelijke, interactieve chatinterface met bronvermeldingen.

### Een praktisch voorbeeld
Stel je een medewerker voor die op zoek is naar een detail in het beleidshandboek van het bedrijf. In plaats van handmatig te zoeken:
- Stelt hij/zij een vraag in natuurlijke taal.
- Het systeem haalt de meest relevante stukken op uit de documentendatabase.
- De LLM genereert een beknopt, gedocumenteerd antwoord.

Deze workflow bespaart tijd, verbetert de nauwkeurigheid en bouwt vertrouwen op door te laten zien waar het antwoord vandaan komt.

### Waarom dit belangrijk is
- Privacy en compliance: Houdt bedrijfseigen gegevens binnen het bedrijfsnetwerk.
- Vertrouwen: Antwoorden zijn gebaseerd op daadwerkelijke documenten met bronvermeldingen.
- Flexibiliteit: Wissel modellen uit, breid documenttypen uit of schaal eenvoudig.

**Klaar voor de praktijk: Al inzetbaar voor kennisbeheer, beleidsopzoekingen of onderzoeksworkflows.

<img src="/images/query-processing.png" alt="Query Processing" width="800"/>

<Callout type="info" title="Source Code">
The entire source code is on my <a href="https://github.com/TAMustafa/LangchainRAG_Streamlit/tree/main" target="_blank">Github</a>
</Callout>

</ProjectLayout>
